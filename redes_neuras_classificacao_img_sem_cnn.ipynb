{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoaoPedro8807/computer_vision/blob/main/redes_neuras_classificacao_img_sem_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_w3owg6ceCm"
      },
      "source": [
        "# **EXTRAÇÃO DOS DADOS DE IMAGENS PARA ENVIAR PARA REDES NEURAIS (SEM CNN POR ENQUANTO APENAS OS VETORES)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f2CzaVU8bm1B"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OuTPGuT8cpeP",
        "outputId": "9bd3b62b-173b-4c7d-a45c-d60373271fd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.10.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = './dados/Datasets/homer_bart_1.zip'\n",
        "\n",
        "zip_object = zipfile.ZipFile(file=data_path, mode='r')\n",
        "zip_object.extractall('./')\n",
        "zip_object.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUqWmIbXeC0B"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkpXE-44dsvp",
        "outputId": "de41940e-3922-41d3-8978-10849ea7080e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "root_path = './homer_bart_1'\n",
        "\n",
        "person_classes = [\n",
        "  ('homer', 0),\n",
        "  ('bart', 1)\n",
        "  ]\n",
        "arquivos = [os.path.join(root_path, f) for f in os.listdir(root_path)]\n",
        "type(arquivos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "MM-haXNhfJa0",
        "outputId": "0865b7da-a89a-4cd5-cdf9-a1e76d38c910"
      },
      "outputs": [],
      "source": [
        "altura, largura = 128, 128\n",
        "altura * largura\n",
        "imagens = []\n",
        "classes = []\n",
        "\n",
        "\n",
        "for arquivo in arquivos:\n",
        "  try:\n",
        "    imagem = cv2.imread(arquivo, cv2.IMREAD_GRAYSCALE)\n",
        "    imagem = cv2.resize(imagem, (altura, largura))\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "  #cv2.imshow(\"Imagem\",imagem)\n",
        "  print(arquivo)\n",
        "  \n",
        "  person_name = arquivo.split('/')[-1].split('\\\\')[-1].split('.')[0]\n",
        "  person_name = [s for s in person_name if s.isalpha()]\n",
        "  person_name = ''.join(person_name)\n",
        "  imagem = imagem.ravel() #transforma a matriz dos pixels em apenas um vetor, para inputar na rede neural\n",
        "\n",
        "  person_class = 1\n",
        "  for clazz in person_classes:\n",
        "    if clazz[0] == person_name:\n",
        "      person_class = clazz[1]\n",
        "\n",
        "  classes.append(person_class)\n",
        "  imagens.append(imagem)\n",
        "\n",
        "  print(person_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "6zAS2SDclC1X"
      },
      "outputs": [],
      "source": [
        "X = np.asarray(imagens)\n",
        "y = np.asarray(classes)\n",
        "\n",
        "#sns.countplot(x=y)\n",
        "total_per_person = np.unique(y, return_counts=True)\n",
        "homer_count, bart_count = total_per_person[1][0], total_per_person[1][1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDREVWo4qI9v"
      },
      "source": [
        "# **TRATANDO DADOS E CRIANDO O MODELO**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naKsQqKiZh1f"
      },
      "source": [
        "TRANSFORMA OS VALORES DOS PIXELS DE 255 PARA UM RANGE DE 0 A 1, PARA MELHORAR O PROCESSAMENTO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bATCe81lpGPM",
        "outputId": "555e7765-c1a9-48b4-f42e-ac5d3daf19d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0 1.0000000000000002\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(X_scaled.min(), X_scaled.max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPUs disponíveis: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "WARNING:tensorflow:From C:\\Users\\joaog\\AppData\\Local\\Temp\\ipykernel_22492\\2028612304.py:6: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "TensorFlow está usando GPU: True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "\n",
        "if gpus:\n",
        "    print(f\"GPUs disponíveis: {gpus}\")\n",
        "    print(f\"TensorFlow está usando GPU: {tf.test.is_gpu_available()}\")\n",
        "else:\n",
        "    print(\"Nenhuma GPU encontrada pelo TensorFlow.\")\n",
        "\n",
        "gpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGWg3eU3ZfBe",
        "outputId": "542d923a-4ee2-4bd9-e249-ff64177e8e08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((54, 16384), (54,))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_treino, x_teste, y_treino, y_teste = train_test_split(X_scaled, y, test_size=0.2, random_state= 1) #garante que os tests e treino serão feitos com os mesmos registros (já separados para test e treino)\n",
        "x_treino.shape, y_treino.shape\n",
        "x_teste.shape, y_teste.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "LMX4olMuaxeu",
        "outputId": "dc1f9739-d259-49ae-e664-2dc5f42ffd4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 8193)              134242305 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8193)              67133442  \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 8194      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 201,383,941\n",
            "Trainable params: 201,383,941\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "7/7 [==============================] - 2s 52ms/step - loss: 130.4283 - accuracy: 0.4791\n",
            "Epoch 2/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 29.3598 - accuracy: 0.4884\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 5.6096 - accuracy: 0.5814\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 1.4437 - accuracy: 0.5628\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.8178 - accuracy: 0.5209\n",
            "Epoch 6/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.7230 - accuracy: 0.4930\n",
            "Epoch 7/100\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 0.6599 - accuracy: 0.6372\n",
            "Epoch 8/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.6420 - accuracy: 0.6326\n",
            "Epoch 9/100\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 0.6687 - accuracy: 0.6047\n",
            "Epoch 10/100\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 0.6401 - accuracy: 0.6558\n",
            "Epoch 11/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.6268 - accuracy: 0.6651\n",
            "Epoch 12/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.6074 - accuracy: 0.6558\n",
            "Epoch 13/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.6193 - accuracy: 0.6605\n",
            "Epoch 14/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.5758 - accuracy: 0.6930\n",
            "Epoch 15/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.6273 - accuracy: 0.6698\n",
            "Epoch 16/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 0.7548 - accuracy: 0.5488\n",
            "Epoch 17/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.7102 - accuracy: 0.6186\n",
            "Epoch 18/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 0.6316 - accuracy: 0.7302\n",
            "Epoch 19/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.5304 - accuracy: 0.7163\n",
            "Epoch 20/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.7757 - accuracy: 0.7163\n",
            "Epoch 21/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.8629 - accuracy: 0.5488\n",
            "Epoch 22/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.6163 - accuracy: 0.6744\n",
            "Epoch 23/100\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 0.5665 - accuracy: 0.6558\n",
            "Epoch 24/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.5912 - accuracy: 0.6651\n",
            "Epoch 25/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.8164 - accuracy: 0.5721\n",
            "Epoch 26/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.6337 - accuracy: 0.6698\n",
            "Epoch 27/100\n",
            "7/7 [==============================] - 0s 45ms/step - loss: 0.5632 - accuracy: 0.7302\n",
            "Epoch 28/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.5487 - accuracy: 0.7349\n",
            "Epoch 29/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.5477 - accuracy: 0.7023\n",
            "Epoch 30/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.4361 - accuracy: 0.7907\n",
            "Epoch 31/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.3971 - accuracy: 0.8558\n",
            "Epoch 32/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.3758 - accuracy: 0.8186\n",
            "Epoch 33/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.3521 - accuracy: 0.8372\n",
            "Epoch 34/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.3600 - accuracy: 0.8140\n",
            "Epoch 35/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.2508 - accuracy: 0.8837\n",
            "Epoch 36/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.2778 - accuracy: 0.8698\n",
            "Epoch 37/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.1577 - accuracy: 0.9349\n",
            "Epoch 38/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.3340 - accuracy: 0.8465\n",
            "Epoch 39/100\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 0.4790 - accuracy: 0.7209\n",
            "Epoch 40/100\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.2725 - accuracy: 0.8698\n",
            "Epoch 41/100\n",
            "7/7 [==============================] - 0s 50ms/step - loss: 0.2613 - accuracy: 0.8837\n",
            "Epoch 42/100\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 0.1813 - accuracy: 0.9116\n",
            "Epoch 43/100\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 0.1207 - accuracy: 0.9628\n",
            "Epoch 44/100\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 0.3477 - accuracy: 0.8372\n",
            "Epoch 45/100\n",
            "7/7 [==============================] - 0s 52ms/step - loss: 0.2987 - accuracy: 0.8651\n",
            "Epoch 46/100\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 0.5867 - accuracy: 0.7581\n",
            "Epoch 47/100\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 0.2286 - accuracy: 0.9023\n",
            "Epoch 48/100\n",
            "7/7 [==============================] - 0s 53ms/step - loss: 0.1504 - accuracy: 0.9581\n",
            "Epoch 49/100\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 0.0809 - accuracy: 0.9953\n",
            "Epoch 50/100\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 0.0520 - accuracy: 0.9953\n",
            "Epoch 51/100\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.0334 - accuracy: 1.0000\n",
            "Epoch 52/100\n",
            "7/7 [==============================] - 0s 49ms/step - loss: 0.0319 - accuracy: 1.0000\n",
            "Epoch 53/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.0226 - accuracy: 1.0000\n",
            "Epoch 54/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0207 - accuracy: 1.0000\n",
            "Epoch 55/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0191 - accuracy: 1.0000\n",
            "Epoch 56/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0123 - accuracy: 1.0000\n",
            "Epoch 57/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0130 - accuracy: 1.0000\n",
            "Epoch 58/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0092 - accuracy: 1.0000\n",
            "Epoch 59/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 60/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 62/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 63/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 64/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 65/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0032 - accuracy: 1.0000\n",
            "Epoch 66/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 67/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 68/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.0030 - accuracy: 1.0000\n",
            "Epoch 69/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 70/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 76/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 79/100\n",
            "7/7 [==============================] - 0s 40ms/step - loss: 0.0013 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 9.6981e-04 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 9.4196e-04 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 8.7526e-04 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 8.4878e-04 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 8.0150e-04 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 7.8200e-04 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 7.5171e-04 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 7.1807e-04 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 7.1595e-04 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 6.6463e-04 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 6.5077e-04 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 6.3634e-04 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 6.0330e-04 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 5.8229e-04 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 5.6304e-04 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "7/7 [==============================] - 0s 39ms/step - loss: 5.4819e-04 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "7/7 [==============================] - 0s 42ms/step - loss: 5.4856e-04 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "7/7 [==============================] - 0s 41ms/step - loss: 5.1390e-04 - accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "qtde_total_pixels = int(x_treino.shape[1])\n",
        "unidades_neuronios = int((qtde_total_pixels + len(person_classes)) / len(person_classes))\n",
        "\n",
        "network1 = tf.keras.models.Sequential()\n",
        "\n",
        "network1.add(tf.keras.layers.Input(shape=(qtde_total_pixels,)))\n",
        "network1.add(tf.keras.layers.Dense(units=unidades_neuronios, activation='relu'))\n",
        "network1.add(tf.keras.layers.Dense(units=unidades_neuronios, activation='relu'))\n",
        "network1.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "network1.summary()\n",
        "\n",
        "#optimizer=adam pq é o padrão para esse tipo de problema simples de 2 classes\n",
        "#loss function = binary_crossentropy utilizado para calcular erro em modelos de classes binária (esse caso)\n",
        "network1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = network1.fit(x_treino, y_treino, epochs=100)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXuhd0lafp2S"
      },
      "source": [
        "# **TESTANDO E METRIFICANDO O MODELO**\n",
        "AGORA USANDO A PARTE DA BASE DE DADOS PARA TESTS\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1cgcGgp9fj_Y",
        "outputId": "4700aab2-4322-444c-d668-6070520efbec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(54, 16384)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "history.history # histórico das 50 epochs\n",
        "\n",
        "predict1 = network1.predict(x_teste)\n",
        "# 1 = True (homer)\n",
        "predict1 = (predict1 > 0.5).astype(bool) # > 0.5 = homer else = bart\n",
        "x_teste.shape\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "qEZ5Z_mDg_iz",
        "outputId": "753027e2-6b7f-4055-9442-0a84fa5be054"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accurancy: 0.7037037037037037\n",
            "confusion matrix: [[17  9]\n",
            " [ 7 21]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accurancy = accuracy_score(predict1, y_teste)\n",
        "cm = tf.math.confusion_matrix(labels=y_teste, predictions=predict1)\n",
        "print(f\"accurancy: {accurancy}\")\n",
        "print(f\"confusion matrix: {cm}\")\n",
        "sns.heatmap(cm, annot=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teri3TT3ioCL",
        "outputId": "01d7cd6d-6b74-4a0b-b0e8-da96837bfb5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.65      0.68        26\n",
            "           1       0.70      0.75      0.72        28\n",
            "\n",
            "    accuracy                           0.70        54\n",
            "   macro avg       0.70      0.70      0.70        54\n",
            "weighted avg       0.70      0.70      0.70        54\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_teste, predict1)) #recall = quanto de cada classe ele conseguiu identificar, e precision a precisão dessa identificacao do recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aSBg654ipr2"
      },
      "source": [
        "SALVANDO OS DADOS DO TREINAMENTO DO MODELO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NZpnug8gjDGx"
      },
      "outputs": [],
      "source": [
        "model_json = network1.to_json()\n",
        "with(open('network1.json', 'w') as f):\n",
        "  f.write(model_json)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGRbrO0zjuo-"
      },
      "source": [
        "SALVA OS PESOS DA REDE NEURAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jCN608J9jnVs"
      },
      "outputs": [],
      "source": [
        "from keras.models import save_model\n",
        "network1.save('network1.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2QOr8HukH1p"
      },
      "source": [
        "Carregando os arquivos salvos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "QpW_RwC4kKGp",
        "outputId": "5a1fc127-2cad-420b-be02-db84f5a341f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\"class_name\": \"Sequential\", \"config\": {\"name\": \"sequential\", \"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 16384], \"dtype\": \"float32\", \"sparse\": false, \"ragged\": false, \"name\": \"input_1\"}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 8193, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_1\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 8193, \"activation\": \"relu\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}, {\"class_name\": \"Dense\", \"config\": {\"name\": \"dense_2\", \"trainable\": true, \"dtype\": \"float32\", \"units\": 1, \"activation\": \"sigmoid\", \"use_bias\": true, \"kernel_initializer\": {\"class_name\": \"GlorotUniform\", \"config\": {\"seed\": null}}, \"bias_initializer\": {\"class_name\": \"Zeros\", \"config\": {}}, \"kernel_regularizer\": null, \"bias_regularizer\": null, \"activity_regularizer\": null, \"kernel_constraint\": null, \"bias_constraint\": null}}]}, \"keras_version\": \"2.10.0\", \"backend\": \"tensorflow\"}'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with(open('network1.json', 'r') as f):\n",
        "  json_save_model = f.read()\n",
        "\n",
        "json_save_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-m1JJNOzlUIl"
      },
      "outputs": [],
      "source": [
        "network1_loaded = tf.keras.models.model_from_json(json_save_model)\n",
        "network1_loaded.load_weights('network1.keras')\n",
        "network1_loaded.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "Y4AeZ0ajmTrS",
        "outputId": "77987993-4331-4b76-8d6a-f0b464044890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 8193)              134242305 \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 8193)              67133442  \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 8194      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 201,383,941\n",
            "Trainable params: 201,383,941\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "network1_loaded.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwYobZWBmMNY"
      },
      "source": [
        "IMPORTANDO O TREINAMENTO E O PESO DE UM .KERAS (MAIS RECOMENDADO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "ba77eec0",
        "outputId": "813954ac-be9e-44a6-acb7-c8c206b039cd"
      },
      "outputs": [
        {
          "ename": "InternalError",
          "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m network1_loaded_from_keras \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnetwork1.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m network1_loaded_from_keras\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m network1_loaded_from_keras\u001b[38;5;241m.\u001b[39msummary()\n",
            "File \u001b[1;32mc:\\Users\\joaog\\OneDrive\\Documentos\\visao_computacional\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\joaog\\OneDrive\\Documentos\\visao_computacional\\venv\\lib\\site-packages\\keras\\backend.py:4302\u001b[0m, in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   4300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m tf\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[0;32m   4301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, value \u001b[38;5;129;01min\u001b[39;00m tuples:\n\u001b[1;32m-> 4302\u001b[0m         \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m get_graph()\u001b[38;5;241m.\u001b[39mas_default():\n",
            "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "network1_loaded_from_keras = tf.keras.models.load_model('network1.keras')\n",
        "network1_loaded_from_keras.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "network1_loaded_from_keras.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URMZQkmddrDN"
      },
      "source": [
        "CLASSIFICAÇÃO DE UNICA IMG\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "collapsed": true,
        "id": "16UgySbJdpok",
        "outputId": "109f6a0a-9b9d-4a5a-a430-71851e20d88a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 32ms/step\n",
            "homer\n"
          ]
        }
      ],
      "source": [
        "img_index = 7\n",
        "img1 = x_teste[img_index]\n",
        "img1 = scaler.inverse_transform(img1.reshape(1, -1))\n",
        "cv2.imshow('img', img1.reshape(128, 128))\n",
        "predict = network1.predict(img1)[0][0]\n",
        "int_class = np.round(predict)\n",
        "for p in person_classes:\n",
        "  if p[1] == int_class:\n",
        "    print(p[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vj8w5TenAJt"
      },
      "source": [
        "# **EXTRAÇÃO DE CARACTERÍSITCAS DA IMAGEM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "3VK2RKRgnFSX",
        "outputId": "e50f4d56-090e-4de6-c74d-55be07a80297"
      },
      "outputs": [],
      "source": [
        "arquivos = [os.path.join(root_path, f) for f in sorted(os.listdir(root_path))]\n",
        "export = 'boca, calca, sapatos, camisa, calcao, tenis, classe\\n'\n",
        "mostrar_img  = False\n",
        "caracteristicas = []\n",
        "\n",
        "for arqv in arquivos:\n",
        "  try:\n",
        "    img_original = cv2.imread(arqv)\n",
        "    (H, W) = img_original.shape[:2]\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "  img_alterada = img_original.copy()\n",
        "  img_caracteristicas = []\n",
        "  img_nome =  os.path.basename(os.path.normpath(arqv))\n",
        "  if img_nome.startswith('h'):\n",
        "    classe = 0 #homer\n",
        "  else:\n",
        "    classe = 1 #bart\n",
        "  boca, calca, sapato, camisa, calcao, tenis = 0, 0, 0, 0, 0, 0\n",
        "  for altura in range(0, H):\n",
        "    for largura in range(0, W):\n",
        "    # RGB -> BGR\n",
        "      azul = img_alterada.item(altura, largura, 0)\n",
        "      verde = img_alterada.item(altura, largura, 1)\n",
        "      vermelho = img_alterada.item(altura, largura, 2)\n",
        "\n",
        "      # Homer - marrom da boca\n",
        "      if (azul >= 95 and azul <= 140 and verde >= 160 and verde <= 185 and vermelho >= 175 and vermelho <= 205):\n",
        "        img_alterada[altura, largura] = [0, 255, 255]\n",
        "        boca += 1\n",
        "\n",
        "      # Homer - azul da calça\n",
        "      if (azul >= 150 and azul <= 180 and verde >= 98 and verde <= 120 and vermelho >= 0 and vermelho <= 90):\n",
        "        img_alterada[altura, largura] = [0, 255, 255]\n",
        "        calca += 1\n",
        "\n",
        "      # Homer - cinza dos sapatos\n",
        "      if altura > (H / 2):\n",
        "        if (azul >= 25 and azul <= 45 and verde >= 25 and verde <= 45 and vermelho >= 25 and vermelho <= 45):\n",
        "          img_alterada[altura, largura] = [0, 255, 255]\n",
        "          sapato += 1\n",
        "\n",
        "      # Bart - laranja da camisa\n",
        "      if (azul >= 11 and azul <= 50 and verde >= 85 and verde <= 105 and vermelho >= 240 and vermelho <= 255):\n",
        "        img_alterada[altura, largura] = [0, 255, 128]\n",
        "        camisa += 1\n",
        "\n",
        "      # Bart - azul do calção\n",
        "      if (azul >= 125 and azul <= 170 and verde >= 0 and verde <= 12 and vermelho >= 0 and vermelho <= 20):\n",
        "        img_alterada[altura, largura] = [0, 255, 128]\n",
        "        calcao += 1\n",
        "\n",
        "      # Bart - azul do tênis\n",
        "      if altura > (H / 2):\n",
        "        if (azul >= 125 and azul <= 170 and verde >= 0 and verde <= 12 and vermelho >= 0 and vermelho <= 20):\n",
        "          img_alterada[altura, largura] = [0, 255, 128]\n",
        "          tenis += 1\n",
        "\n",
        "  boca = round((boca / (H * W)) * 100, 9)\n",
        "  calca = round((boca / (H * W)) * 100, 9)\n",
        "  sapato = round((boca / (H * W)) * 100, 9)\n",
        "  camisa = round((camisa / (H * W)) * 100, 9)\n",
        "  calcao = round((calcao / (H * W)) * 100, 9)\n",
        "  tenis = round((tenis / (H * W)) * 100, 9)\n",
        "\n",
        "  img_caracteristicas.append(boca)\n",
        "  img_caracteristicas.append(calca)\n",
        "  img_caracteristicas.append(sapato)\n",
        "  img_caracteristicas.append(camisa)\n",
        "  img_caracteristicas.append(calcao)\n",
        "  img_caracteristicas.append(tenis)\n",
        "  img_caracteristicas.append(classe)\n",
        "  caracteristicas.append(img_caracteristicas)\n",
        "\n",
        "  #print(f\"VALOR HOMER BOCA: {img_caracteristicas[0]}\\n VALOR HOMER CALCA: {img_caracteristicas[1]}\\n VALOR SAPATO: {img_caracteristicas[2]}\", )\n",
        "\n",
        "  f = ','.join([str(c) for c in img_caracteristicas])\n",
        "  export += f + '\\n'\n",
        "  if mostrar_img:\n",
        "    img_alterada = cv2.cvtColor(img_alterada, cv2.COLOR_BGR2RGB)\n",
        "    img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img_original)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(img_alterada)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "export\n",
        "with open('features.csv', 'w') as f:\n",
        "    f.write(export)\n",
        "f.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = pd.read_csv('features.csv')\n",
        "ds.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 6.88610189, 3.49520435,\n",
              "       3.49520435])"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = ds.iloc[:, :-1].values #caracteristicas\n",
        "y = ds.iloc[:, -1].values #rotulos\n",
        "len(X), len(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# separação 70% treino / 30% teste (mantendo distribuição de classes com stratify)\n",
        "x_treino, x_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
        "\n",
        "# normaliza características (MinMax 0-1)\n",
        "scaler_features = MinMaxScaler()\n",
        "x_treino = scaler_features.fit_transform(x_treino)\n",
        "x_teste = scaler_features.transform(x_teste)\n",
        "\n",
        "# verificações rápidas\n",
        "print(\"shapes:\", x_treino.shape, x_teste.shape, y_treino.shape, y_teste.shape)\n",
        "print(\"train distribution:\", np.unique(y_treino, return_counts=True))\n",
        "print(\"test distribution:\", np.unique(y_teste, return_counts=True))\n",
        "x_treino[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "network2 = tf.keras.models.Sequential()\n",
        "\n",
        "network2.add(tf.keras.layers.Input(shape=(x_treino.shape[1], )))  \n",
        "network2.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "network2.add(tf.keras.layers.Dense(units=8, activation='relu'))\n",
        "network2.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "network2.summary()\n",
        "network2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history2 = network2.fit(x_treino, y_treino, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3/3 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[ True],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [ True],\n",
              "       [False],\n",
              "       [False]])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict2 = network2.predict(x_teste)\n",
        "predict2 = (predict2 > 0.5).astype(bool)\n",
        "predict2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8271604938271605"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accurancy = accuracy_score(predict2, y_teste)\n",
        "accurancy"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPmXn07br1dv2tIXARkaHXE",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
